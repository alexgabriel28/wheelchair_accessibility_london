{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48014fe3-9926-4583-9990-8fc803b0707e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Purpose\n",
    "This report presents the analysis of the venues in London which are equipped with a wheelchair ramp. </p>\n",
    "A dataset with ~4000 observations was used, mapping different venue attributes to the binary variable of having a wheelchair ramp or not. </p>\n",
    "A model was then developed to predict whether a venue already has a wheelchair ramp. The prediction accuracy is ~65%. True-negative-rate > 66%. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3253447-20ec-41bb-bfc4-6636d5c33eaa",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "The marketing department aims at expanding the potential market for wheelchair installations to event venues. </p>\n",
    "Reportedly, 40 % of event venues already have a wheelchair ramp. </p>\n",
    "Due to reputation concerns and resource restrictions, it is unfeasible to query every venue with regards to the existence of a ramp.</p>\n",
    "Therefore, aiming to optimize the lead generation process, an approriate model has to be developed to predict whether or not a venue already has a ramp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be8632-f973-4fd1-8f29-1ca585f5e2cc",
   "metadata": {},
   "source": [
    "### Preliminary Remarks\n",
    "The terms _variable_ and _feature_ are used interchangeably. </p>\n",
    "</p>\n",
    "The following independent variables are given\n",
    "<ul>\n",
    "  <li>venue_name : The name of the venue</li>\n",
    "  <li>Loud music / events : whether the venue hosts loud events (True) or\n",
    "not (False)</li>\n",
    "  <li>Wi-Fi : Numeric, whether the venue provides alcohol (1) or not (0)</li>\n",
    "  <li>supervenue : Character, whether the venue qualifies as a supervenue\n",
    "(True) or not (False).</li>\n",
    "  <li>max_standing :  Numeric, the total standing capacity of the venue.</li>\n",
    "  <li>Theatre_max : Numeric, the total capacity of the theatre.</li>\n",
    "  <li>Promoted / ticketed events : Character, whether the venue hosts promoted/ticket\n",
    "events (True) or not (False).</li>\n",
    "</ul></p>\n",
    "The following dependent variable is given\n",
    "<ul>\n",
    "    <li>Wheelchair accessible : Character, whether the venue is wheelchair accessible\n",
    "        (True) or not (False).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f7df6-3fa9-4de1-93c4-16d83ac2bcf4",
   "metadata": {},
   "source": [
    "### Analysis Structure\n",
    "The report is structured in three parts:</p>\n",
    "An __EDA__, description of the __Model Development__ and a __Conclusion__.</p>\n",
    "Special emphasis is put on reproducibility and explainability of the result derivation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4d42c-36df-4e3f-86f3-1ab010197eef",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-hampshire",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "     |████████████████████████████████| 262 kB 28.4 MB/s            \n",
      "\u001b[?25hCollecting pydantic>=1.8.1\n",
      "  Downloading pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
      "     |████████████████████████████████| 12.6 MB 2.4 MB/s            \n",
      "\u001b[?25hCollecting visions[type_image_path]==0.7.4\n",
      "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "     |████████████████████████████████| 102 kB 20.0 MB/s           \n",
      "\u001b[?25hCollecting seaborn>=0.10.1\n",
      "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "     |████████████████████████████████| 292 kB 104.1 MB/s            \n",
      "\u001b[?25hCollecting phik>=0.11.1\n",
      "  Downloading phik-0.12.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (696 kB)\n",
      "     |████████████████████████████████| 696 kB 104.7 MB/s            \n",
      "\u001b[?25hCollecting pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3\n",
      "  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     |████████████████████████████████| 11.7 MB 101.1 MB/s            \n",
      "\u001b[?25hCollecting tqdm>=4.48.2\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 19.2 MB/s            \n",
      "\u001b[?25hCollecting missingno>=0.4.2\n",
      "  Downloading missingno-0.5.1-py3-none-any.whl (8.7 kB)\n",
      "Collecting multimethod>=1.4\n",
      "  Downloading multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
      "Collecting jinja2>=2.11.1\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     |████████████████████████████████| 133 kB 113.2 MB/s            \n",
      "\u001b[?25hCollecting numpy>=1.16.0\n",
      "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "     |████████████████████████████████| 16.8 MB 76.9 MB/s            \n",
      "\u001b[?25hCollecting markupsafe~=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting requests>=2.24.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "     |████████████████████████████████| 63 kB 3.6 MB/s             \n",
      "\u001b[?25hCollecting htmlmin>=0.1.12\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting joblib~=1.1.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 91.0 MB/s            \n",
      "\u001b[?25hCollecting tangled-up-in-unicode==0.2.0\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "     |████████████████████████████████| 4.7 MB 96.2 MB/s            \n",
      "\u001b[?25hCollecting scipy>=1.4.1\n",
      "  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
      "     |████████████████████████████████| 41.6 MB 92.2 MB/s            \n",
      "\u001b[?25hCollecting matplotlib>=3.2.0\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 88.5 MB/s            \n",
      "\u001b[?25hCollecting PyYAML>=5.0.0\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "     |████████████████████████████████| 701 kB 82.1 MB/s            \n",
      "\u001b[?25hCollecting attrs>=19.3.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "     |████████████████████████████████| 60 kB 19.7 MB/s            \n",
      "\u001b[?25hCollecting networkx>=2.4\n",
      "  Downloading networkx-2.8-py3-none-any.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 88.2 MB/s            \n",
      "\u001b[?25hCollecting imagehash\n",
      "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
      "     |████████████████████████████████| 812 kB 95.3 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-9.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 87.2 MB/s            \n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 13.6 MB/s            \n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 18.2 MB/s             \n",
      "\u001b[?25hCollecting python-dateutil>=2.7\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     |████████████████████████████████| 247 kB 106.3 MB/s            \n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "     |████████████████████████████████| 930 kB 95.7 MB/s            \n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 97.2 MB/s            \n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "     |████████████████████████████████| 503 kB 90.4 MB/s            \n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "     |████████████████████████████████| 138 kB 90.8 MB/s            \n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "     |████████████████████████████████| 149 kB 110.7 MB/s            \n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 15.3 MB/s            \n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting PyWavelets\n",
      "  Downloading PyWavelets-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 66.1 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: htmlmin, imagehash\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=4821702ec48738c7ea82269d5d22e8a5ba384abbe2fed6602d8b8b6b89d021ec\n",
      "  Stored in directory: /home/repl/.cache/pip/wheels/23/14/6e/4be5bfeeb027f4939a01764b48edd5996acf574b0913fe5243\n",
      "  Building wheel for imagehash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=afd309206a571740d1507d570c601253d9ed98541cf89b65225154bd9b19864a\n",
      "  Stored in directory: /home/repl/.cache/pip/wheels/48/a1/7f/096c1269d6bf78d4768180602579b35a1e8cb1250bb4b40c74\n",
      "Successfully built htmlmin imagehash\n",
      "Installing collected packages: six, pyparsing, pytz, python-dateutil, Pillow, packaging, numpy, kiwisolver, fonttools, cycler, tangled-up-in-unicode, scipy, PyWavelets, pandas, networkx, multimethod, matplotlib, attrs, visions, urllib3, typing-extensions, seaborn, markupsafe, joblib, imagehash, idna, charset-normalizer, certifi, tqdm, requests, PyYAML, pydantic, phik, missingno, jinja2, htmlmin, pandas-profiling\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to begin, and add as many cells as you need to complete your analysis!\n",
    "# Perform necessary imports and installs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install pandas-profiling\n",
    "!pip install optuna\n",
    "!pip install lime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import phik\n",
    "from phik.report import plot_correlation_matrix\n",
    "from phik import report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068de08-ee00-4483-8bff-b293e6aa4aa2",
   "metadata": {},
   "source": [
    "First, the variables are checked for dtypes. __Alcohol__ is converted to dtype _bool_.</p>\n",
    "The variable names are adjusted for better consistency.</p>\n",
    "The dataset is checked for missing values (none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb2278-9d7d-49d7-ad6c-c66d298cd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data into Dataframe and describe numerical data\n",
    "venues = pd.read_csv(\"data/event_venues.csv\")\n",
    "venues.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38fa1c-08e4-49fa-bfae-c3756e664dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency and clarity\n",
    "venues = venues.rename(columns = {\n",
    "    \"venue_name\":\"Venue\",\n",
    "    \"Loud music / events\": \"Loud Music\",\n",
    "    \"Venue provides alcohol\": \"Alcohol\",\n",
    "    \"supervenue\": \"Supervenue\",\n",
    "    \"U-Shaped_max\":\"U Cap\",\n",
    "    \"max_standing\":\"Standing Cap\",\n",
    "    \"Theatre_max\":\"Max Cap\",\n",
    "    \"Promoted / ticketed events\": \"Promo Events\",\n",
    "    \"Wheelchair accessible\":\"WC Ramp\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584942b7-7b73-436e-98c7-71855332a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change \"Alcohol\" to binary type\n",
    "venues[\"Alcohol\"] = venues[\"Alcohol\"].astype(bool)\n",
    "\n",
    "# Change the remaining numeric variables to dtype = int. (Capacity of humans in float doesn't make sense)\n",
    "venues[[\"U Cap\", \"Standing Cap\", \"Max Cap\"]] = venues[[\"U Cap\", \"Standing Cap\", \"Max Cap\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660d72e-f97e-4808-9460-55465847b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for na values\n",
    "venues.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8276da-cb7c-4e4c-955d-872b3143506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = venues.duplicated().sum()\n",
    "uniques = len(pd.unique(venues.Venue))\n",
    "print(f\"Duplicate observations: {duplicates} | Unique venues: {uniques}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163873ea-2f80-4b3e-9d66-69da13aa81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe\n",
    "# Keep only the first entry for each duplicate, drop the rest\n",
    "total_ven_ramp = venues[\"WC Ramp\"].sum()/len(venues)\n",
    "venues = venues.sort_values(by = \"Venue\").drop_duplicates()\n",
    "print(f\"{total_ven_ramp:.2f} % of events had ramps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13211da-ed97-48f6-84cc-f48035d4fd4c",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "<ul>\n",
    "    <li>The dataset has 3910 observations. </li> \n",
    "    <li>450 (11.51 %) are duplicates. </li>\n",
    "    <li>1444 (36.93 %) venues are unique. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc043ff-23e1-44eb-b1af-f22f690c1cc3",
   "metadata": {},
   "source": [
    "The same venue can have different capacities and attributes. For example, one venue can allow alcohol on one occasion and prohibit it on another. Therefore, there are less unique venues than unique observations in the dataset. </p>\n",
    "__3__ variables are __numeric__ and real-valued: \"U Cap\", \"Standing Cap\" and \"Max Cap\", describing the capacities of the venue </p>\n",
    "__6__ are __binary__: \"Loud Music\", \"Alcohol\", \"Wi-Fi\", \"Supervenue\", \"Promo Events\", \"WC Ramp\" </p>\n",
    "Dependent variable: __\"WC Ramp\"__ (specifies whether a wheelchair ramp is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f60a2-28cf-495f-b26b-c121eb61c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many venues vary with respect to having a wheelchair ramp, how many always have it, how many never have it\n",
    "check_fractions = venues.groupby(\"Venue\")[\"WC Ramp\"].sum()/venues.groupby(\"Venue\")[\"WC Ramp\"].count()\n",
    "ven_var_ramp = ((check_fractions != 0.0) & (check_fractions != 1.0)).sum()\n",
    "ven_with_ramp = ((check_fractions == 1.0)).sum()\n",
    "ven_wo_ramp = ((check_fractions == 0.0)).sum()\n",
    "\n",
    "assert ven_var_ramp + ven_with_ramp + ven_wo_ramp == len(check_fractions), \"Ratios must sum to 1\"\n",
    "\n",
    "#print(f\"{ven_var_ramp} venues ({ven_var_ramp/len(check_fractions):.2f} %) are inconsistent with regards to providing a wheelchair ramp.\")\n",
    "#print(f\"{ven_with_ramp} venues ({ven_with_ramp/len(check_fractions):.2f} %) always have a ramp.\")\n",
    "#print(f\"{ven_wo_ramp} venues ({ven_wo_ramp/len(check_fractions):.2f} %) never have a ramp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a05d97-d78e-4a8d-9f70-8f47ca5dd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of unique venues that have a wheelchair ramp (including those with inconsistent use)\n",
    "unique_venues = venues.drop_duplicates(subset=[\"Venue\", \"WC Ramp\"])\n",
    "weelchair_ratio = unique_venues[\"WC Ramp\"].sum()/len(unique_venues)\n",
    "#print(f\"{weelchair_ratio*100:.2f} % of venues have a wheelchair ramp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c0edf-2855-47f6-a01a-9d6d38282dc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How many venues have ramps?\n",
    "In summary:</p>\n",
    "Of the unique venues, __38 %__ __always__ have a ramp. </p>\n",
    "__46 %__ of the venues __never__ have a ramp. </p>\n",
    "__16 %__ are __inconsistent__ in their use. </p>\n",
    "__50 %__ of the individual __events__ had ramps. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ace74-a316-4ce4-b8bd-92f978bb3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ramp frequencies\n",
    "sns.set(font_scale = 1, style = \"white\")\n",
    "fig, ax = plt.subplots(figsize = (6, 5))\n",
    "ramp_frac = pd.DataFrame([ven_wo_ramp/len(check_fractions), ven_with_ramp/len(check_fractions),  ven_var_ramp/len(check_fractions)])\n",
    "ramp_frac = ramp_frac.rename(index = {0:\"Without Ramp\", 1:\"With Ramp\", 2:\"Inconsistent\"}, columns = {0:\"Frequency\"})\n",
    "sns.barplot(y = ramp_frac.index, x = \"Frequency\", data = ramp_frac, orient = \"h\", ax = ax)\n",
    "ax.set_title(\"46 % of venues don't have a ramp.\\n16 % report inconsistent use.\", pad = 20)\n",
    "plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff295b-068b-408b-a95e-333e893910e2",
   "metadata": {},
   "source": [
    "Duplicate observations have been removed from the dataset.  \n",
    "Venues that have inconsistent entries with regards to having a ramp are kept.  \n",
    "More information on the causation of the inconsistent entries would be necessary to decide which observations to keep and which to drop.  \n",
    "One hypothesis could be that the inconsistent venues didn't have a ramp at first, but subsequently added it, or that temporary ramps were available for certain events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0294ea-fd9d-4890-baa8-61e89b59114d",
   "metadata": {},
   "source": [
    "## Distributions of variables\n",
    "The distribution of the numerical and binary variables is explored more in-depth to understand the dataset.  \n",
    "Relationships between the independent variables and/or the dependent variable are investigated to prepare the model development and identify related variables.</p>\n",
    "The __goal__ is to understand __which variables indicate__ that a ramp is __already installed__ at a venue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7751e69-b7f8-4705-994d-708d97261e3f",
   "metadata": {},
   "source": [
    "### Numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea173c97-6720-49a5-ad15-faf630f7ec31",
   "metadata": {},
   "source": [
    "The pandas description of the numeric variables in the dataset shows that the capacity of the venues varies greatly. </p>\n",
    "For the variable __Standing Cap__, the __mean__ value (112.9) is more than __double__ the __median__ value (60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7f0df-5c1d-45fb-927b-f9fd1b8139ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary description of numeric variables\n",
    "numericals = venues.select_dtypes(include = \"number\")\n",
    "numericals.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab73ae-09f9-47c0-ad37-270b0c6fc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get median \n",
    "numericals.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388f345-45df-4856-a5cd-df61d46297cb",
   "metadata": {},
   "source": [
    "Investigation of the __ECDF__ plots of the numerical variables suggests __exponential distributions__, with most values in the lower range and a few outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0d57d-f595-4803-85d7-13f013fbbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ECDFs\n",
    "fig, axs = plt.subplots(1, 3, sharey = True, figsize = (10, 5))\n",
    "for i, col in enumerate(numericals.columns):\n",
    "\tsns.ecdfplot(venues[col], ax = axs[i])\n",
    "\taxs[i].set_xlabel(col + \" [-]\")\n",
    "\n",
    "axs[0].set_ylabel(\"ECDF [%]\")\n",
    "fig.suptitle(\"The numeric variables are exponentially distributed\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb9b98-2bb6-4870-bf6d-35deb044dc17",
   "metadata": {},
   "source": [
    "Analysis of the box plots confirms this view. The outliers of the boxplot below have been omitted to increase visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa9728-265f-4a10-b765-ea10c4ddf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals[\"WC Ramp\"] = venues[\"WC Ramp\"]\n",
    "numericals_melted = pd.melt(numericals, id_vars = \"WC Ramp\", var_name = \"Cap Type\", value_name = \"Capacity\").sort_values(\"WC Ramp\", ascending = False)\n",
    "\n",
    "ax = sns.boxplot(x = \"Cap Type\", y = \"Capacity\", hue = \"WC Ramp\", data = numericals_melted, showfliers = False)\n",
    "ax.set_title(f\"Venues with a ramp have a higher variance of capacity. \\n Venues with a ramp seem to have higher standing capacity.\")\n",
    "ax.set_ylabel(\"Capacity [-]\")\n",
    "ax.set_xlabel(\"Capacity Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde1a00-d2ad-420a-ab5d-d2c5f803c13e",
   "metadata": {},
   "source": [
    "It is odd, that __U Cap__ shows almost not variance.  \n",
    "Therefore the values in __U Cap__ are counted, showing that in __73 %__ of the cases, the __U Cap__ value is __35__. The next most frequent value is 30 with only 3 % frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10533b95-e702-4850-9f3b-bc20c23dcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "venues[\"U Cap\"].value_counts()/len(venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad436d22-73ee-49ea-9bfd-2d18d52f1bb7",
   "metadata": {},
   "source": [
    "A possible hypothesis for this could be that part of the dataset was derived from an online form, where 35 could be specified as the lowest U-Cap number.  \n",
    "Another possibility is that the dataset is corrupted. Since this is not provable from the given information, the data is not changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20bd6f-2a18-4b19-9834-d116d85d5ea4",
   "metadata": {},
   "source": [
    "We further check for consistency with __Max Cap__.  \n",
    "__U Cap__ must be strictly smaller than __Max Cap__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5e206-1d47-47be-8967-ba2f68eb82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_larger_max = venues[\"Max Cap\"] < venues[\"U Cap\"]\n",
    "print(f\"In {u_larger_max.sum()} cases, the given U-Cap value is larger than the Max Cap value\")\n",
    "venues.loc[u_larger_max, \"U Cap\"] = venues.loc[u_larger_max, \"Max Cap\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae18bb-7f35-45a7-a8b8-0421c3e320e4",
   "metadata": {},
   "source": [
    "192 observations don't satisfy the requirement. The __U Cap__ value for those observations is set to the value of __Max Cap__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dacb1a-9b64-4d71-8ddb-39a49848af99",
   "metadata": {},
   "source": [
    "From plotting the single variables in a jointplot (not shown here), we can observe that __large venues__ have a ramp in most cases and define a large venue as: </p>\n",
    "U-Shape Capacity > 200  \n",
    "Standing Capacity > 500  \n",
    "Maximum Capacity > 200  \n",
    "\n",
    "To test whether the means between large and small venues are different, the columns are split at the given thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca6ccb-2f4c-422b-b1e7-a77403869e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get means for large and small events\n",
    "means = pd.DataFrame(columns = numericals.drop(\"WC Ramp\", axis = 1).columns)\n",
    "thresh = [200, 500, 200]\n",
    "\n",
    "# Store means for large and small events and get diff of means\n",
    "for i, col in enumerate(means.columns):\n",
    "\tmeans.loc[0, col] = round(numericals.loc[numericals[col] > thresh[i], \"WC Ramp\"].mean(), 2)\n",
    "\tmeans.loc[1, col] = round(numericals.loc[numericals[col] <= thresh[i], \"WC Ramp\"].mean(), 2)\n",
    "\tmeans.loc[2, col] = means.loc[0, col] - means.loc[1, col]\n",
    "means[\"Venue Size\"] = [\"large\", \"small\", \"diff\"]\n",
    "means = means.set_index(\"Venue Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93783f6-3e20-4eda-a50a-5cbdf98d933c",
   "metadata": {},
   "source": [
    "The mean probability of having a  ramp at a large venue is almost double as high as at smaller venues.</p>\n",
    "The mean differences can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3e9bd-fe07-49a5-a2e2-2553157da402",
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ddf39-a160-4b06-8e33-da0e0d98b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to test for difference in test statistics\n",
    "def bootstrap_diff_metric(data_1, data_2, func, diff_value, draws = 10000) -> [list, float, list]:\n",
    "    \"\"\" \n",
    "    This function trains the given classifiers.\n",
    "    Args:\n",
    "        data_1: array_like: the first array with data to test. \n",
    "        data_2: array_like: the second array, being subtracted from func(data_1)\n",
    "        func: function: test_metric with signature func(data_1, data_2) and single-value return\n",
    "        diff_value: int: the test statistic from the empirical data to be tested against\n",
    "        draws: int (Default = 10000): number of random draws\n",
    "    Returns:\n",
    "        output: [list, float, list]: returns a tuple of the confidence intervals, the p_value and the samples\n",
    "    \"\"\"      \n",
    "    samples = []\n",
    "    data = np.concatenate((data_1, data_2))\n",
    "    overall_mean = data.mean()\n",
    "    data_1 = data_1 - data_1.mean() + overall_mean\n",
    "    data_2 = data_2 - data_2.mean() + overall_mean\n",
    "    for _ in range(draws):\n",
    "        diff_sample = func(np.random.choice(data_1, size = len(data_1))) - func(np.random.choice(data_2, size = len(data_2)))\n",
    "        samples.append(diff_sample)\n",
    "    p_value = np.sum(np.array(samples) >= diff_value)/draws\n",
    "    ci = np.percentile(samples, [2.5, 97.5])\n",
    "    return ci, p_value, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a2a46-1779-4c22-92ff-7ac72c6366ee",
   "metadata": {},
   "source": [
    "The hypothesis that __large venues__ have more wheelchair ramps than small venues was checked next.</p>\n",
    "__H0: mean(large) - mean(small) = 0__</p>\n",
    "For this, the datasets for large and small venues were merged and the overall mean calculated. The datasets were then equalized to have the same mean.</p>\n",
    "The bootstraps are drawn __pseudo-randomly__ from the merged and equalized dataset.  \n",
    "A total of __10,000 bootstraps__  each was performed.  \n",
    "The __p-value__ and __95 % confidence-intervals__ are returned by the function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715eda3b-bdf2-4c53-a0ca-f3f6fb2171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value and ci for U-Cap\n",
    "ucap_ci, ucap_p, ucap_bs = bootstrap_diff_metric(\n",
    "    venues.loc[venues[\"U Cap\"] > 200, \"WC Ramp\"], \n",
    "    venues.loc[venues[\"U Cap\"] <= 200, \"WC Ramp\"], \n",
    "    np.mean, \n",
    "    means.iloc[2, 0])\n",
    "ucap_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c8da6-c6b7-439f-996b-0dd161f60680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value and ci for Standing Cap\n",
    "stcap_ci, stcap_p, stcap_bs = bootstrap_diff_metric(\n",
    "    venues.loc[venues[\"Standing Cap\"] > 1000, \"WC Ramp\"], \n",
    "    venues.loc[venues[\"Standing Cap\"] <= 1000, \"WC Ramp\"], \n",
    "    np.mean, \n",
    "    means.iloc[2, 1])\n",
    "stcap_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750436c-ac49-434c-b4f6-e250eaa5d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value and ci for Standing Cap\n",
    "maxcap_ci, maxcap_p, maxcap_bs = bootstrap_diff_metric(\n",
    "    venues.loc[venues[\"Max Cap\"] > 700, \"WC Ramp\"], \n",
    "    venues.loc[venues[\"Max Cap\"] <= 700, \"WC Ramp\"], \n",
    "    np.mean, \n",
    "    means.iloc[2, 2])\n",
    "maxcap_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba562be-ad34-44e2-8066-0e6d312c7d3d",
   "metadata": {},
   "source": [
    "__p-values__</p>\n",
    "The difference is significant in all for __Standing Cap__ and __Max Cap__, with p-values (might vary slightly):</p>\n",
    "<ul>\n",
    "    <li>U Cap: 0.06</li>\n",
    "    <li>Standing Cap: 0.0007</li>\n",
    "    <li>Max Cap: 0.0</li>\n",
    "</ul>\n",
    "U-Cap is slightly not significant (5 % limit), but consideration of the effect should not be discarded.</p>\n",
    "We can conclude that large venues (as defined above) are relatively more likely to have wheelchair ramps than smaller venues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4607b-5460-4ada-b121-2ceed55aff99",
   "metadata": {},
   "source": [
    "### Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c0f14-1a40-4555-938f-569d2efa0e62",
   "metadata": {},
   "source": [
    "The binary variable __Large Venue__ is therefore introduced into the __venues__ dataframe for further analysis and potential improvement of prediction accuracy.</p>\n",
    "After a short view of the descriptor statistics of the capacities of __Supervenue__, it is clear that __Supervenue__ does __not__ indicate venue size or capacity</p>\n",
    "This was an initial misconception that is stated here for general clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48ecfe-74d2-4161-871a-60cdc38f6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "venues.groupby(\"Supervenue\")[[\"Max Cap\", \"U Cap\", \"Standing Cap\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef49e0-a744-46bf-89eb-aff31e2c4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary variable Large Venue\n",
    "venues[\"Large Venue\"] = ((venues[\"U Cap\"] > thresh[0]) | (venues[\"Standing Cap\"] > thresh[1]) | (venues[\"Max Cap\"] > thresh[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b2506f-e10e-4b96-bb8c-2dd028603c8c",
   "metadata": {},
   "source": [
    "In addition, a feature __Area__ is generated from the available numeric features to account for the strong correlation of the numerical values. (see \"Relations between variables\")</p>\n",
    "__Assumptions__:\n",
    "<ul>\n",
    "    <li>Each seat in the U-Shaped theather accounts for an area with a radius of 2 meters.</li>\n",
    "    <li>Each standing place accounts for a radius of 0.5 meters.</li>\n",
    "    <li>Seats in Max Cap have a radius of 2 meter as well. </li>\n",
    "</ul>\n",
    "Area = pi * r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3744b-ffca-458e-8e45-80b19ed04f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Add area as additional feature\n",
    "venues[\"Area\"] = venues[\"U Cap\"]*math.pi*2**2 + venues[\"Standing Cap\"]*math.pi*0.5**2 + (venues[\"Max Cap\"] - venues[\"U Cap\"]) * math.pi*1.5**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c886b-5249-4e38-ac9d-161afb7f30cd",
   "metadata": {},
   "source": [
    "Following up on the findings, the difference of the median standing capacities for venues with and without wheelchair ramps is evaluated.</p>\n",
    "The median is chosen over the mean to mitigate outlier influence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8db35-532a-4494-82ed-d41e748fc87d",
   "metadata": {},
   "source": [
    "The median __Standing Cap__ and __Area__ values for venues with and without a wheelchair are given in the table below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd626c-b4bc-4313-9201-b8d1daffa9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = venues.groupby(\"WC Ramp\")[[\"Standing Cap\", \"Area\"]].median()\n",
    "medians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75a175-0f14-461b-9d93-cd52c31c5c94",
   "metadata": {},
   "source": [
    "Having a wheelchair ramp seems to be tied to a __high Standing Cap__.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54167ae9-2adb-45c8-b1b5-fa1d16a84775",
   "metadata": {},
   "source": [
    "10,000 value sets for __Standing Cap__ were bootstrapped and the difference of medians calculated, as previously described.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f9daf-eebf-4de9-a21a-e358f1f84f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "stcap_ci, stcap_p, stcap_bs = bootstrap_diff_metric(\n",
    "    venues.loc[venues[\"WC Ramp\"] == True, \"Standing Cap\"], \n",
    "    venues.loc[venues[\"WC Ramp\"] == False, \"Standing Cap\"], \n",
    "    np.median, \n",
    "    medians.loc[1, \"Standing Cap\"] - medians.loc[0, \"Standing Cap\"]\n",
    ")\n",
    "stcap_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1db28-4e19-432f-8d60-683d6b7d0b5d",
   "metadata": {},
   "source": [
    "The difference __is significant__.  \n",
    "The __p-values__ is __0.0__ </p>\n",
    "As the above values are consistent, __we can conclude that large venues, (esp. standing places) are significantly more likely to have a wheelchair ramp than small venues__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de246792-c82f-4a64-84e8-00b2a2e39776",
   "metadata": {},
   "source": [
    "### Binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f89036-207e-4135-b600-a643758a4c6a",
   "metadata": {},
   "source": [
    "Looking at the __binary variables__, we can draw the following conclusions:\n",
    "<ul>\n",
    "  <li>38 % of venues have Loud Music</li>\n",
    "  <li>74 % of venues serve Alcohol</li>\n",
    "  <li>93 % of venues provide Wi-Fi</li>\n",
    "  <li>6 % of venues are Supervenues</li>\n",
    "    <li>60 % are Promo Events </li>\n",
    "    <li>6 % are Large Venues </li>\n",
    "</ul></p>\n",
    "The distribution for the binary variables is depicted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8407f-c9ef-48ef-b2ee-23b6fa964a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaries = venues.select_dtypes(include = \"bool\").drop(\"WC Ramp\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb07e-55ac-4488-85bb-88895cc6370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "fig, axs = plt.subplots(1, len(binaries.columns), figsize = (10, 4), sharey = True)\n",
    "for i, col in enumerate(binaries.columns):\n",
    "    sns.histplot(binaries[col], ax = axs[i], stat = \"probability\", binwidth = 0.25)\n",
    "    axs[i].set_xticks([0.0, 1.0])\n",
    "    axs[i].set_xticklabels([\"False\", \"True\"])\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"The vast majority of venues serves alcohol and provides Wifi. There are only a few supervenues.\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc45ea-57a6-4fe6-819d-1e73367f37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequencies for binary variables\n",
    "freq = []\n",
    "for col in binaries.columns:\n",
    "\tfreq.append(round(binaries[[col]].value_counts(normalize = True).mul(100),2).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0ca95-a37d-4093-aa4b-a0901e0ee7c3",
   "metadata": {},
   "source": [
    "## Relations between variables\n",
    "Next follows the analysis of the relations between the variables, as a preparation for model development. </p>\n",
    "The conditional probability of a venue having a wheelchair ramp, given that the variable is True is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc4c2a-46dd-45fe-9dcb-7b539f3b375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of ramps given variable\n",
    "venues_bool = venues.select_dtypes(\"bool\")\n",
    "venues_wc_attribute = venues_bool[venues_bool[\"WC Ramp\"] == True].drop(\"WC Ramp\", axis = 1).sum()\n",
    "total_venues_attribute = venues_bool.drop(\"WC Ramp\", axis = 1).sum()\n",
    "wc_ratio = round(venues_wc_attribute/total_venues_attribute*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7ee20-cee9-4c10-85f8-efca8c5031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of ramps given the venues does not have the attribute\n",
    "venues_bool_neg = ~venues.select_dtypes(\"bool\")\n",
    "venues_wc_attribute_neg = venues_bool_neg[venues_bool_neg[\"WC Ramp\"] == True].drop(\"WC Ramp\", axis = 1).sum()\n",
    "total_venues_attribute_neg = venues_bool_neg.drop(\"WC Ramp\", axis = 1).sum()\n",
    "wc_ratio_neg = round(venues_wc_attribute_neg/total_venues_attribute_neg*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad42d31-597d-42fe-975f-2f72340d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditionals = pd.DataFrame(wc_ratio.append(wc_ratio_neg), columns = [\"Ratio [%]\"])\n",
    "conditionals[\"With Feature\"] = [\"Yes\"]*int(len(conditionals)/2) + [\"No\"]*int(len(conditionals)/2)\n",
    "conditionals = conditionals.sort_values(by = \"Ratio [%]\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804061d8-fd5e-490a-84e6-7d54c950a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "sns.barplot(y = \"Ratio [%]\", x = conditionals.index, hue = \"With Feature\", data = conditionals, ax = ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title = \"Does the venue have the feature?\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Frequency [%]\")\n",
    "plt.title(\"Promo Events have a ramp more often than not \\n Supervenues often don't have one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe954a9-e085-4fc8-8b81-22f01985f55b",
   "metadata": {},
   "source": [
    "To check, if the difference of means of installed ramps at supervenues is significant, we used the function from above, receiving a p-value of 0.0.  \n",
    "Indeed, the average of installed ramps at supervenues is significantly smaller than at normal venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78d49c-52c2-4b7f-ac03-c8b955ce0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = venues.loc[venues[\"Supervenue\"] == False, \"WC Ramp\"].sum() - venues.loc[venues[\"Supervenue\"] == True, \"WC Ramp\"].sum()\n",
    "super_ci, super_p, super_bx = bootstrap_diff_metric(\n",
    "    venues.loc[venues[\"Supervenue\"] == False, \"WC Ramp\"],\n",
    "    venues.loc[venues[\"Supervenue\"] == True, \"WC Ramp\"],\n",
    "    np.mean,\n",
    "    mean_diff,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c40a3-b4c6-4153-8bda-cd140b6b825b",
   "metadata": {},
   "source": [
    "From this we can already isolate two important variables:\n",
    "_supervenue_ is an indicator for __not__ having a wheelchair ramp: P(ramp | Supervenue) = 30.29 % </p>\n",
    "_Promo Events_ have a wheelchair ramp more often than not: P(ramp | Promo Event) = 60.64 % </p>\n",
    "Venues without Wi-Fi also seem to have a ramp more often.</p>\n",
    "For the remaining variables, there is a ramp approx. as often as not, given they are True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28c16b-8068-478f-803a-32fdc41fb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine a df numerics\n",
    "numerics = venues.select_dtypes(include = \"number\")\n",
    "numerics[\"WC Ramp\"] = venues[\"WC Ramp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687f38e-3c9f-476b-b111-ec989525199b",
   "metadata": {},
   "source": [
    "The correlation matrix for venues is visualized as a heatmap. </p>\n",
    "The dataset contains binary and numerical variables with strong outliers. The Pearson correlation is not the best metric for this, because:</p>\n",
    "<ul>\n",
    "  <li>Limited to continuous variables</li>\n",
    "  <li>Only accounts for linear relationship</li>\n",
    "  <li>Sensitive to outliers</li>\n",
    "</ul></p>\n",
    "Instead, the __phi_k coefficient__ is chosen as a metric to measure the correlation between the variables. </p>\n",
    "(cp: <href>https://arxiv.org/abs/1811.11440</href>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaff01-584a-451c-994d-3576d988127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the heatmap of the phi_k correlation of the variables\n",
    "# means.columns is taken from the pen-ultimate code-cell to specify the interval_cols\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "venues = venues.reindex((sorted(venues.drop(\"WC Ramp\", axis = 1).columns, reverse = True)).insert(0, \"WC Ramp\"), axis=1)\n",
    "sns.heatmap(round(venues.drop(\"Venue\", axis = 1).phik_matrix(interval_cols = venues.select_dtypes(\"number\").columns), 2), annot = True, ax = axs[0])\n",
    "axs[0].set_title(\"Correlation Matrix (phi_k)\")\n",
    "\n",
    "significance_overview = venues.drop(\"Venue\", axis = 1).significance_matrix(interval_cols=venues.select_dtypes(\"number\").columns)\n",
    "sns.heatmap(round(significance_overview[[\"WC Ramp\"]].drop(\"WC Ramp\", axis = 0), 1), annot = True, vmin = -5, vmax = 5, square = \"equal\", cmap = \"seismic\", ax = axs[1])\n",
    "axs[1].set_title(\"Significance of Coefficients\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f367f39-8d66-457e-b9da-355dd778e686",
   "metadata": {},
   "source": [
    "The left plot shows the Correlation matrix. </p> In addition to that, the significance of the correlation coefficients with the dependent variable \"WC Ramp\" is plotted to the right. The right colormap saturates at +/- 5 standard deviations.</p>\n",
    "We can verify that especially the __correlation__ with __Large Venue__, __Promo Events__ and __Area__ is __significant__.</p>\n",
    "Except for \"Loud Music\" and \"Wi-Fi\", all values are > 5*std</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2226ab-865b-4b0b-a830-c25b868028d4",
   "metadata": {},
   "source": [
    "The variables that are \"strongest\" correlated to __WC Ramp__ are:  \n",
    "<ol>\n",
    "  <li>Large Venue: 0.27 (+)</li>\n",
    "  <li>Promo Events: 0.21 (+)</li>\n",
    "  <li>Area: 0.17 (+)</li>\n",
    "  <li>Supervenue: 0.17 (-)</li>\n",
    "  <li>Alcohol: 0.16 (+)</li>\n",
    "</ol></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd367b7-ef95-4f7d-989d-52c04347ccd0",
   "metadata": {},
   "source": [
    "Further remarks:</p>\n",
    "<ul>\n",
    "    <li>Promo Events and Loud Music are strongly correlated: 0.5</li>\n",
    "    <li>Loud Music and Alcohol are correlated: 0.37</li>\n",
    "    <li>Max Cap and Standing Cap are strongly correlated: 0.89</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565f1c3-363f-4dc8-b49e-a83f01f9494c",
   "metadata": {},
   "source": [
    "It is to note, that the __correlations with WC Ramp__ are __weak__.</p>\n",
    "The phi_k coefficient does not indicate direction, as the Pearson correlation does.  \n",
    "The direction given above is inferred from the previous analysis.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98510d96-55f6-40df-ba69-44a1dd48edb7",
   "metadata": {},
   "source": [
    "Furthermore, the created variable __Area__ seems to change with __Loud Music__ and __Alcohol__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f2cf96-72ae-4497-86a6-d04dd66f0643",
   "metadata": {},
   "source": [
    "An additional column is created:  \n",
    "__Party__ := True, iff Loud Music == True or Promo Events == True or Alcohol == True </p>\n",
    "The correlation with WC Ramp is __0.2152__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6594ec-bcaa-48df-969c-0dfaec1067dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "venues[\"Party\"] = venues[\"Promo Events\"] | venues[\"Loud Music\"] | venues[\"Alcohol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d2863-be46-4c37-806a-4692b4eb8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(venues[[\"WC Ramp\", \"Party\"]].phik_matrix(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01804616-6aa7-481f-bff0-d1d23e4fa544",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "The goal is to predict if a given venue already has a ramp. </p>\n",
    "Minimum requirements:  \n",
    "True-Negative-Rate > 2/3  \n",
    "High Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d394318-9fdc-4e3a-9ea8-a3e36955b1dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methods  \n",
    "First, a vanilla __Random Forest Classifier__, a __Gradient Boost Classifier__, __K-Nearest Neighbors Classifer__ and a __Support Vector Classifier__ are fit to the dataset. If possible, all have random_state = 3078.   The model search space is limited to shallow algorithms, as they are generally better suited for tabular data than artificial neural networks (resource-intensive, bad interpretability)</p>\n",
    "The numeric features are scaled with sklearn's __StandardScaler()__. (This is not necessary for the ensemble / tree methods, although sometimes beneficial.) </p>\n",
    "All independent features (variables) are used for this.</p>\n",
    "__Data-Split__: The dataset is split up __0.8 : 0.2__ (train:test) </p>\n",
    "__Feature importance__ is measured. We derive the most important features to decrease dimensionality. </p>\n",
    "__Hyperparameter optimization__ is executed. The __Optuna__ library is used for this. 100 trials are executed. (see Appendix)</p>\n",
    "The best model params are retrieved and used to evaluate scores on a cross_validated dataset. </p>\n",
    "The reported metrics are: __confusion matrix__, __accuracy__, __precision__, __recall__, __f1-score__. Accuracy is taken from 5-fold stratified cross-validation on the 80 % training data. </p>\n",
    "The final model is trained on only 5 features on the 80 % training dataset, with competitive results to the model trained on the entire dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859b250-ebd8-421d-9571-431aee9a1e05",
   "metadata": {},
   "source": [
    "(Preliminary experiments with an XGBoostClassifier were executed. This didn't render any improvement to the sklearn classifiers and was thus omitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c9ba6-7a04-4a9c-ab96-8359da460bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import sklearn\n",
    "from typing import Any\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report, precision_score, average_precision_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79555c91-3bc0-4f81-b5de-d42677875342",
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = venues.set_index(\"Venue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8cd319-f3f2-4724-9dc6-9c1d6c0b97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix_scorer from sklearn-website adapted:\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "# Define training function\n",
    "def train_classifier(clf_params_dict: dict,\n",
    "                     X: pd.DataFrame,\n",
    "                     y: pd.DataFrame,\n",
    "                     scaler: Any = None,\n",
    "                     train_size: float = 0.8,\n",
    "                    ) -> dict:\n",
    "    \"\"\" \n",
    "    This function trains the given classifiers.\n",
    "    Args:\n",
    "        clf_params_dict: dict: with signature: {\"clf_name\":{\"clf\":sklearn_model, \"params\":dict(params)}\n",
    "        X: pd.DataFrame: the training data\n",
    "        y: pd.DataFrame: the labels\n",
    "        scaler: Any (optional): a sklearn scaler object\n",
    "        train_size: float (Default=0.8): the train_size for train_test_split\n",
    "    Returns:\n",
    "        output: Dict: a dict with signature \n",
    "                    {\"clf_name\":{\"conf_matrix\":confusion_matrix, \"accuracy\":accuracy, \"model\":trained model}}\n",
    "    \"\"\"   \n",
    "    # Split dataset in train_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = train_size)\n",
    "    \n",
    "    if scaler is not None:\n",
    "        X_index = X_train.select_dtypes(\"number\").columns\n",
    "        X_train[X_index] = scaler.fit_transform(X_train[X_index])\n",
    "        X_test[X_index] = scaler.transform(X_test[X_index])\n",
    "    \n",
    "    # Train classifiers and store model, accuracy and confusion matrix in dict output\n",
    "    output = {}\n",
    "    for k, v in clf_params_dict.items():\n",
    "        clf = v[\"clf\"]\n",
    "        if v[\"params\"] is not None:\n",
    "            clf.set_params(**v[\"params\"])\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        \n",
    "        # Note: cross_validate is used three times here. For faster execution this can be strongly optimized.\n",
    "        score = cross_val_score(clf, X_train, y_train, n_jobs = -1, cv = 5, scoring = \"f1\")\n",
    "        accuracy = score.mean()\n",
    "        accuracy = (accuracy_score(y_test, preds) + accuracy)/2\n",
    "\n",
    "        score_conf = cross_validate(clf, X_train, y_train, n_jobs = -1, cv = 5, scoring = confusion_matrix_scorer)\n",
    "        matrix = np.array([0, 0, 0, 0])\n",
    "        matrix[0] = score_conf[\"test_tn\"].mean()\n",
    "        matrix[1] = score_conf[\"test_fp\"].mean()\n",
    "        matrix[2] = score_conf[\"test_fn\"].mean()\n",
    "        matrix[3] = score_conf[\"test_tp\"].mean()\n",
    "        matrix = matrix.reshape(2,2)\n",
    "        conf_matrix = matrix/matrix.sum(axis = 1)\n",
    "        \n",
    "        scoring = {\"precision\":\"average_precision\", \"recall\":\"recall\", \"f1\":\"f1\"}\n",
    "        metrics = cross_validate(clf, X_train, y_train, n_jobs = -1, cv = 5, scoring = {\"precision\":\"average_precision\", \"recall\":\"recall\", \"f1\":\"f1\"})\n",
    "        \n",
    "        output[k] = {\"conf_matrix\":0, \"accuracy\":0, \"model\":0}\n",
    "        output[k][\"conf_matrix\"] = conf_matrix\n",
    "        output[k][\"accuracy\"] = accuracy\n",
    "        output[k][\"model\"] = clf\n",
    "        output[k][\"metrics\"] = metrics\n",
    "        \n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8525b7-61ef-43c9-8b10-5b11e2323424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature importances\n",
    "def plot_feature_importances(\n",
    "    clf_dict: dict,\n",
    "    feat_import: pd.DataFrame,\n",
    ") -> Any:\n",
    "    \"\"\" \n",
    "    This function plots the feature importances of sklearn models, that have the attribute\n",
    "    feature_importances_\n",
    "    Args:\n",
    "        clf_dict: a dictionary with keys: classifier names, values: trained classifiers\n",
    "        feat_import: a DataFrame with the Feature Names in the first column\n",
    "    Returns:\n",
    "        A plot of the feature importances\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize = (6, 5))\n",
    "    for k, v in clf_dict.items():\n",
    "        feat_import[k] = pd.Series(v.feature_importances_)\n",
    "\n",
    "    feat_import_melt = pd.melt(\n",
    "        feat_import, \n",
    "        id_vars = \"Feature\", \n",
    "        value_vars = clf_dict.keys(), \n",
    "        value_name = \"Importances\",\n",
    "        var_name = \"Classifier\"\n",
    "    )\n",
    "    sns.barplot(\n",
    "        x = \"Importances\", \n",
    "        y = \"Feature\", \n",
    "        hue = \"Classifier\", \n",
    "        data = feat_import_melt.sort_values(\"Importances\", ascending = False),\n",
    "        orient = \"h\",\n",
    "        ax = ax)\n",
    "    most_important = feat_import_melt[\n",
    "        feat_import_melt[\"Importances\"] == feat_import_melt[\"Importances\"].max()\n",
    "    ][\"Feature\"].to_numpy()\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "    ax.set_ylabel(\"Importance\")\n",
    "    fig.suptitle(f\"The feature '{most_important.squeeze()}' Has Highest Influence on the Prediction\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50c398-9073-48b9-b3c3-d3d2ace08b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    conf_matrix_dict: dict,\n",
    "    title: str = None,\n",
    ") -> Any:\n",
    "    \"\"\" \n",
    "    This function plots the confusion matrices as heatmaps\n",
    "    Args:\n",
    "        conf_matrix_dict: a dictionary with keys: classifier names, values: confusion matrices\n",
    "        title: An optional plot title\n",
    "    Returns:\n",
    "    \tA plot of the confusion matrices\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, len(conf_matrix_dict), sharey = True, figsize = (10, 4))\n",
    "    i = 0\n",
    "    for k, v in conf_matrix_dict.items():\n",
    "        if i < len(conf_matrix_dict) - 1:\n",
    "        \tsns.heatmap(v, ax = axs[i], annot = True, xticklabels = [\"Negative\", \"Positive\"], cbar = False)\n",
    "        else:\n",
    "            sns.heatmap(v, ax = axs[i], annot = True, xticklabels = [\"Negative\", \"Positive\"], cbar = True)\n",
    "        axs[i].set_title(k)\n",
    "        axs[i].set_xlabel(\"Prediction\")\n",
    "        i += 1\n",
    "        axs[0].set_yticklabels([\"Negative\", \"Positive\"])\n",
    "        if title is None:\n",
    "        \tfig.suptitle(\"Confusion Matrices\")\n",
    "        else:\n",
    "            fig.suptitle(title)\n",
    "    axs[0].set_ylabel(\"Ground Truth\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07067336-d553-43b4-b443-c07288354bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This might take a while!\n",
    "\n",
    "# Instantiate StandardScaler for numerical values (not obligatory for tree-like, but might be beneficial)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create dataset\n",
    "y = venues[\"WC Ramp\"]\n",
    "X_cols = venues.drop(\"WC Ramp\", axis =1).columns\n",
    "X = venues[X_cols]\n",
    "\n",
    "# Instantiate models\n",
    "forest_clf = RandomForestClassifier(random_state = 3078)\n",
    "gb_clf = GradientBoostingClassifier(random_state = 3078)\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC(random_state = 3078)\n",
    "\n",
    "# Define dict\n",
    "clf_params_dict = {\"KNN\":{\"clf\": knn, \"params\":None},\n",
    "                   \"GBC\":{\"clf\": gb_clf, \"params\":None},\n",
    "                   \"Forest\":{\"clf\": forest_clf, \"params\":None},\n",
    "                   \"SVC\":{\"clf\": svc, \"params\":None},\n",
    "                  }\n",
    "\n",
    "# Train models\n",
    "clf_dict = train_classifier(clf_params_dict, X, y, scaler = StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa426c3c-7673-4fb8-a9f0-c7f85972aa4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Selection\n",
    "The feature importances are plotted below for the tree-based Forest and GradBoost Classifiers.</p>\n",
    "We can see that the feature engineering for __Party__ and __Area__ seems successful.  \n",
    "__Area__ is among the top three most important features.  \n",
    "__Party__ is among the most important binary features for GradBoost.  \n",
    "In general, the numeric features are more important than the binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5964090-8f6b-452f-a435-7fbd22f9df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "clf_dict_fi = {\"GradBoost\":clf_dict[\"GBC\"][\"model\"], \"Forest\":clf_dict[\"Forest\"][\"model\"]}\n",
    "feat_import = pd.DataFrame(X.columns, columns = [\"Feature\"])\n",
    "\n",
    "plot_feature_importances(clf_dict_fi, feat_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a79e9d-2096-4060-aa02-fe4be83fdd35",
   "metadata": {},
   "source": [
    "Next the importance of each feature was analyzed, using PCA. </p>\n",
    "(The code snippet below was gratefully adapted from this great blog post: https://towardsdatascience.com/3-essential-ways-to-calculate-feature-importance-in-python-2f9149592155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efe164-acd4-4d8c-a906-fdb114a3d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(X)\n",
    "X_scaled[X.select_dtypes(\"number\").columns] = scaler.fit_transform(X.select_dtypes(\"number\"))\n",
    "pca = PCA().fit(X_scaled)\n",
    "\n",
    "# Compute the loadings\n",
    "loadings = pd.DataFrame(\n",
    "    data=pca.components_.T * np.sqrt(pca.explained_variance_), \n",
    "    columns=[f'PC{i}' for i in range(1, len(X_scaled.columns) + 1)],\n",
    "    index=X_scaled.columns\n",
    ")\n",
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb5d3f-23bf-4ab1-ab47-c67ea6e60b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loadings\n",
    "pc1_loadings = loadings.sort_values(by=\"PC1\", ascending=False)[[\"PC1\"]]\n",
    "pc1_loadings = pc1_loadings.reset_index()\n",
    "pc1_loadings.columns = [\"Attribute\", \"CorrelationWithPC1\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axs[1].bar(x=pc1_loadings[\"Attribute\"], height=pc1_loadings[\"CorrelationWithPC1\"])\n",
    "axs[1].set_title(\"PCA loading scores (first principal component)\", size=11)\n",
    "axs[1].set_ylabel(\"Loading Score\")\n",
    "axs[1].set_xlabel(\"Feature\")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "\n",
    "axs[0].plot(pca.explained_variance_ratio_.cumsum(), lw=3)\n",
    "axs[0].set_title(\"3 components explain 90 % of variance\", size=11)\n",
    "axs[0].set_ylabel(\"Cum. explained variance\")\n",
    "axs[0].set_xlabel(\"Number of components\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeffbbf-bd67-4d4e-9c19-de79b92e1edd",
   "metadata": {},
   "source": [
    "The above plots confirm the intuition from the feature importance plot.</p>\n",
    "The left plot shows that only 3 components explain 90 % of the variance in the data.</p>\n",
    "The plot to the right shows the correlation of each feature with the first principal component.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b232b-d010-4435-aa64-dc6f04f4d525",
   "metadata": {},
   "source": [
    "## Accuracy and Confusion\n",
    "\n",
    "The confusion matrices for the respective classifiers are visualized below.</p>\n",
    "This provides valuable insight. __GradBoost__ seems to have the __most balanced__ performance with regards to False-Positives and False-Negatives and the best prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e8abc-b388-44b6-b5e4-ffb2fbf1e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for initial classifiers\n",
    "conf_matrix_dict = {\n",
    "    \"KNN\":clf_dict[\"KNN\"][\"conf_matrix\"], \n",
    "    \"GradBoost\":clf_dict[\"GBC\"][\"conf_matrix\"], \n",
    "    \"Forest\":clf_dict[\"Forest\"][\"conf_matrix\"], \n",
    "    \"SVC\": clf_dict[\"SVC\"][\"conf_matrix\"]\n",
    "}\n",
    "\n",
    "plot_confusion_matrix(conf_matrix_dict, \"Comparison of Initial Classifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59de4b2-7835-4d58-ab19-cf2eec2fc5e6",
   "metadata": {},
   "source": [
    "The total accuracy varies between ~48 - 64 % on a 5-fold cross-validation of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c8c09-b35b-453a-a11d-02162bc6e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy for initial classifiers\n",
    "accuracies = {k:v[\"accuracy\"] for k, v in clf_dict.items()}\n",
    "acc = pd.DataFrame.from_dict(accuracies, orient = \"index\", columns = [\"Accuracy [%]\"])\n",
    "ax = sns.barplot(x = acc.index, y = acc.loc[:,\"Accuracy [%]\"], data = acc)\n",
    "ax.set_title(\"The max accuracy is around 64 %\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77300d-0982-4cb0-acd4-5aa86a00be3e",
   "metadata": {},
   "source": [
    "A hyperparameter search was executed, using Optuna. The code is given in the Appendix for reproducibility. </p>\n",
    "Based on the feature analysis, __Max Cap__, __Standing Cap__, __Area__, __U Cap__ and __Promo Events__ are selected going forward. The results of the vanilla classifiers on these features are shown below. Of the numeric features only U-Cap has been omitted.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad76cd7-bf65-4e03-8956-6e1abc3fdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# This might take a while!\n",
    "\n",
    "# Instantiate StandardScaler for numerical values (not obligatory for boosting tree, but might be beneficial)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create dataset\n",
    "y = venues[\"WC Ramp\"]\n",
    "X_cols = [\"U Cap\", \"Promo Events\", \"Area\", \"Max Cap\", \"Standing Cap\"]\n",
    "X = venues[X_cols]\n",
    "\n",
    "# Train models\n",
    "clf_dict_red = train_classifier(clf_params_dict, X, y, scaler = StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7b40a-06e6-4697-9443-b72800061692",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_dict_red = {\n",
    "    \"KNN\":clf_dict_red[\"KNN\"][\"conf_matrix\"], \n",
    "    \"GradBoost\":clf_dict_red[\"GBC\"][\"conf_matrix\"], \n",
    "    \"Forest\":clf_dict_red[\"Forest\"][\"conf_matrix\"], \n",
    "    \"SVC\": clf_dict_red[\"SVC\"][\"conf_matrix\"]\n",
    "}\n",
    "\n",
    "plot_confusion_matrix(conf_matrix_dict_red, \"Grad Boost Shows Best Result on the Reduced Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab3369-211c-4f45-a3f1-e4e150c0d27d",
   "metadata": {},
   "source": [
    "The results are very similar to the initial classifiers, but with only 50 % of the features.</p>\n",
    "This can be a helpful approach to reduce resources and save time for larger datasets and shows that the feature selection was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee7e1e-9ab2-4cf4-a1d7-a2ef2f3efc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best classifier\n",
    "optim_params = {\"max_depth\": 3, 'n_estimators': 98, 'min_samples_split': 2, \"random_state\" : 3078}\n",
    "#{'max_depth': 5, 'n_estimators': 49, 'min_samples_split': 3, \"random_state\":3078}\n",
    "best_clf_dict = train_classifier({\"GBC\":{\"clf\": GradientBoostingClassifier(), \"params\":optim_params}}, X, y, scaler = StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ad527-d85a-42c5-82dc-4db8f4378b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_dict_red = {\n",
    "    \"GradBoost Final\":best_clf_dict[\"GBC\"][\"conf_matrix\"],\n",
    "    \"GradBoost Red\":clf_dict_red[\"GBC\"][\"conf_matrix\"],\n",
    "    \"GradBoost Initial\":clf_dict[\"GBC\"][\"conf_matrix\"],\n",
    "}\n",
    "\n",
    "plot_confusion_matrix(conf_matrix_dict_red, \"Confusion Matrices for the Different Stages of the Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dd70f-cb9b-4968-9c50-0a91b0bc436a",
   "metadata": {},
   "source": [
    "The feature selection drove the model to a __higher true-negative rate__</p>\n",
    "However, the initial model has the most balanced prediction accuracy, with respect to true-negatives and true-positives.</p>\n",
    "The selection of the right model depends on the emphasis being put on either predicting venues without ramps, or with ramps. </p>\n",
    "In general, all three models reach the goal of a True-Negative Rate of > 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270e361-cf9a-4037-bdab-c28200e85cd2",
   "metadata": {},
   "source": [
    "The final accuracies can be seen below.</p>\n",
    "__GradBoostFinal__ and __GradBoost Red__ operate on only __50 % of the features__.  \n",
    "__GradBoostFinal__ is __hyperparameter-tuned__.  \n",
    "__GradBoost Initial__ was trained on __all features__ and with __vanilla hyperparameters__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752b1ac-392f-4d8c-ae3e-3daf296a4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_acc_dict = {\n",
    "    \"GradBoost Final\":best_clf_dict[\"GBC\"][\"accuracy\"],\n",
    "    \"GradBoost Red\":clf_dict_red[\"GBC\"][\"accuracy\"],\n",
    "    \"GradBoost Initial\":clf_dict[\"GBC\"][\"accuracy\"],\n",
    "}\n",
    "accuracies = {k:v for k, v in clf_acc_dict.items()}\n",
    "acc = pd.DataFrame.from_dict(accuracies, orient = \"index\", columns = [\"Accuracy\"])\n",
    "ax = sns.barplot(x = acc.index, y = acc.loc[:,\"Accuracy\"], data = acc)\n",
    "ax.set_title(\"Comparison of the Cross-Val Accuracies\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8770a1a-e622-449c-8ba0-e9f2959e0828",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics_dict = {\"GradBoost Final\":best_clf_dict[\"GBC\"][\"metrics\"],\n",
    "    \"GradBoost Red\":clf_dict_red[\"GBC\"][\"metrics\"],\n",
    "    \"GradBoost Initial\":clf_dict[\"GBC\"][\"metrics\"],\n",
    "    }\n",
    "precisions = {k:v[\"test_precision\"].mean() for k, v in clf_metrics_dict.items()}\n",
    "recalls = {k:v[\"test_recall\"].mean() for k, v in clf_metrics_dict.items()}\n",
    "f1s = {k:v[\"test_f1\"].mean() for k, v in clf_metrics_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786d5fe-0ed9-4f84-84c2-e1d2a08acfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame()\n",
    "metrics = metrics.append(recalls, ignore_index = True).append(precisions, ignore_index = True).append(f1s, ignore_index = True)\n",
    "metrics[\"Metric\"] = [\"Precision\",\"Recall\",\"F1-Score\"]\n",
    "metrics = pd.melt(metrics, id_vars = \"Metric\", value_vars = metrics.drop(\"Metric\", axis = 1).columns, value_name = \"Result\", var_name = \"Classifier\").sort_values(\"Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e893b0-21c6-4a29-8735-3665fa7648c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "sns.scatterplot(y = \"Result\", x = \"Metric\", hue = \"Classifier\", data = metrics, ax = ax)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title = \"Classifier\")\n",
    "plt.title(\"Performance on entire dataset ~ selected dataset \\n Tuning can improve the model. (x-lim = [0.4, 0.8])\", fontsize = 15, pad = 10)\n",
    "ax.set_ylim([0.25, 0.8])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698799ba-7d33-405a-a868-2c8849cb8bd6",
   "metadata": {},
   "source": [
    "## ML Explainer\n",
    "Finally, we want to visualize the influence of our features on the prediction probability of our model.</p>\n",
    "LIME is used as a library to accomplish this (note: SHAP was not working, because of some dependency conflicts with the installed numpy version == 1.22.3)</p>\n",
    "(It might be that this has to be viewed in the Jupyter Notebook, as the new workspace editor did not allow me to visualize .html.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683a85a-5364-4f53-900e-219cbf985b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Prepare LIME explainer\n",
    "scaler = StandardScaler()\n",
    "X_scaled_red = pd.DataFrame(X)\n",
    "X_scaled_red[X_scaled_red.select_dtypes(\"number\").columns] = scaler.fit_transform(X_scaled_red.select_dtypes(\"number\"))\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X),\n",
    "    feature_names=X.columns,\n",
    "    class_names=['False', 'True'],\n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7c8fb-7f3c-4203-a6f7-dabda7c2ecb2",
   "metadata": {},
   "source": [
    "The ML-Explainer shows us the predicted probabilities for a chosen observation -> the observation can be changed by setting the value of _EXAMPLE_. </p>\n",
    "This is generally in line with our previous findings:</p>\n",
    "__High Max Cap__, __Area__ and __Standing Cap__ usually are __good predictors__ for an existing ramp. </p>\n",
    "Further, __Promo Events__ is a __weak predictor__ for having a ramp as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5086145-27ff-49d0-be83-e0d89a0be5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lime explainer for EXAMPLE\n",
    "# This value can be adjusted to vary the example that is visualized\n",
    "EXAMPLE = 150\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=X.iloc[EXAMPLE], \n",
    "    predict_fn=best_clf_dict[\"GBC\"][\"model\"].predict_proba\n",
    ")\n",
    "\n",
    "exp.save_to_file(\"explained.html\")\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818d89f-581a-4532-8dec-c6e1659f3145",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8bb9b-23ef-4c60-a65a-3fc66c361e74",
   "metadata": {},
   "source": [
    "A dataset, containing the mapping of event venues and their attributes to whether they have a wheelchair ramp or not, was analyzed.</p>\n",
    "Background was the pursuit of the Marketing team to expand the business to event venues.</p>\n",
    "For this, a model with a true-negative-rate of at least 2/3 and a test-accuracy of > 60% was developed. </p>\n",
    "</p>\n",
    "The most important features were identified (+/-: relation):\n",
    "<ul>\n",
    "  <li>Maximum Capacity (+)</li>\n",
    "  <li>Standing Capacity (+)</li>\n",
    "  <li>Promo Events (+)</li>\n",
    "  <li>Estimated Venue Area (+)</li>\n",
    "  <li>U-Shape Capacity (+)</li>\n",
    "</ul></p>\n",
    "It can be summarized that large venues are equipped with wheelchair ramps significantly more often than small venues. This is due to the fact that venues with a high standing capacity have a high number of ramps. </p>\n",
    "Only ~30% of supervenues do have a wheelchair ramp. </p>\n",
    "Party venues are usually also already accessible via wheelchair. </p>\n",
    "\n",
    "According to our findings, __the recommendation is, to target small- & mid-sized venues, with few standing capacities, as well as high-quality venus (supervenues).__ For the following pre-selection and search for venues, the following thresholds are given:\n",
    "<ul>\n",
    "    <li>U-Shape Capacity: max. 200 </li>\n",
    "    <li>Standing Capacity: max. 500 </li>\n",
    "    <li>Maximum Theater Capacity: max. 200</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b70092-5c6a-4df0-b6cc-893c4625dada",
   "metadata": {},
   "source": [
    "A corresponding model was fitted and can predict if a venue has a ramp, or not with an accuracy of ~65%.</p>\n",
    "This can help with a pre-selection. Further information, such as exact location of all venues, as well as real areas, could help to improve the model.</p>\n",
    "The model can be adjusted in a certain range to either have a higher true-negative, or true-positive-rate, depending on the emphasis of the Marketing Team.</p>\n",
    "Since model accuracy is still in the lower range, further investigation and data collection should be the next step to improve performance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb768f-979c-46dd-b766-7db946b91b9e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea9003-5649-4b87-a46e-56bf3cb5605d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e8df8-4669-43ce-b291-f10aa4f3a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HYPEROPT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37343f72-f6b6-40ec-aca5-88b84acbedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HYPEROPT:\n",
    "    import sklearn\n",
    "\n",
    "    import optuna\n",
    "\n",
    "    def confusion_matrix_scorer(clf, X, y):\n",
    "            y_pred = clf.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "                    'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "    def objective(trial):\n",
    "\n",
    "        # Suggest values for the hyperparameters using a trial object.\n",
    "        classifier_name = trial.suggest_categorical(\"classifier\", [\"SVC\", \"Forest\", \"GradBoost\"])\n",
    "        if classifier_name == \"SVC\":\n",
    "             svc_c = trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True)\n",
    "             classifier_obj = SVC(C=svc_c, gamma='auto', random_state = 3078)\n",
    "\n",
    "        elif classifier_name == \"Forest\":\n",
    "            rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n",
    "            rf_estimators = trial.suggest_int(\"rf_estimators\", 10, 1e4, log = True)\n",
    "\n",
    "            classifier_obj = RandomForestClassifier(max_depth=rf_max_depth, n_estimators=rf_max_depth, random_state = 3078)\n",
    "\n",
    "        elif classifier_name == \"GradBoost\":\n",
    "            gb_max_depth = trial.suggest_int(\"gb_max_depth\", 3, 100, log = True)\n",
    "            gb_estimators = trial.suggest_int(\"gb_estimators\", 1, 100, log = True)\n",
    "            gb_min_split = trial.suggest_int(\"gb_min_split\", 2, 10, log = False)\n",
    "\n",
    "            classifier_obj = GradientBoostingClassifier(max_depth=gb_max_depth, n_estimators=gb_estimators, min_samples_split=gb_min_split, random_state = 3078)\n",
    "\n",
    "        else:\n",
    "            optimizer = trial.suggest_categorical('algorithm', ['auto','ball_tree','kd_tree','brute'])\n",
    "            knn_neighbors = trial.suggest_int(\"knn_neighbors\", 2, 1e2, log=True)\n",
    "            knn_leafs = trial.suggest_int(\"knn_leafs\", 5, 100, log = True)\n",
    "\n",
    "            knn = KNeighborsClassifier(n_neighbors=knn_neighbors,algorithm=optimizer, leaf_size = knn_leafs)\n",
    "        score = cross_val_score(classifier_obj, X, y, n_jobs=-1, cv=5, scoring = \"accuracy\")\n",
    "        accuracy = score.mean()\n",
    "        return accuracy\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    trial_best = study.best_trial\n",
    "    trial_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ef5dc-b881-4998-9608-72bfa2bf4970",
   "metadata": {
    "tags": []
   },
   "source": [
    "## END OF REPORT"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
